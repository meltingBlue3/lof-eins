---
phase: 01-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/downloader.py
  - src/data/generator/generators.py
autonomous: true

must_haves:
  truths:
    - "announcement_parses table exists with correct schema"
    - "limit_event_log table exists with correct schema"
    - "All tables have proper indexes for query performance"
    - "Migration script can create both tables in existing databases"
  artifacts:
    - path: "src/data/downloader.py"
      provides: "announcement_parses and limit_event_log CREATE TABLE statements"
      pattern: "CREATE TABLE.*announcement_parses"
    - path: "src/data/generator/generators.py"
      provides: "Table creation for announcement_parses and limit_event_log"
      pattern: "announcement_parses"
    - path: "src/data/migrations/001_create_new_tables.py"
      provides: "Migration script for existing databases"
      min_lines: 50
  key_links:
    - from: "announcement_parses.ticker"
      to: "limit_events.ticker"
      via: "foreign key relationship (logical, not enforced)"
    - from: "limit_event_log"
      to: "limit_events"
      via: "audit trail for debugging"
---

<objective>
Create two new database tables: announcement_parses and limit_event_log.

Purpose: These tables support the PDF announcement processing pipeline. announcement_parses stores raw LLM extraction results, while limit_event_log provides an audit trail for debugging timeline changes.

Output: Complete table schemas with indexes, integrated into both downloader.py (for real data) and generators.py (for mock data), plus a migration script for existing databases.
</objective>

<execution_context>
@C:/Users/zhang/.config/opencode/get-shit-done/workflows/execute-plan.md
@C:/Users/zhang/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Table Schemas Required:**

**announcement_parses:**
- id: INTEGER PRIMARY KEY AUTOINCREMENT
- ticker: TEXT NOT NULL (index)
- announcement_date: DATE NOT NULL
- pdf_filename: TEXT NOT NULL
- parse_result: JSON (SQLite TEXT storing JSON)
- parse_type: TEXT (e.g., 'complete', 'open-start', 'end-only')
- confidence: REAL (0.0-1.0)
- processed: INTEGER DEFAULT 0 (index, boolean flag)
- created_at: TIMESTAMP DEFAULT CURRENT_TIMESTAMP

**limit_event_log:**
- id: INTEGER PRIMARY KEY AUTOINCREMENT
- ticker: TEXT NOT NULL (index)
- operation: TEXT NOT NULL (e.g., 'INSERT', 'UPDATE', 'MERGE')
- old_start: DATE
- old_end: DATE
- new_start: DATE
- new_end: DATE
- triggered_by: TEXT (announcement ID or reason)
- created_at: TIMESTAMP DEFAULT CURRENT_TIMESTAMP

**Indexes needed:**
- announcement_parses: ticker, processed
- limit_event_log: ticker, created_at
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create announcement_parses table in downloader.py</name>
  <files>src/data/downloader.py</files>
  <action>
    Add a new method `_create_announcement_parses_table()` to RealDataDownloader class and call it from `_generate_limit_db()` or create a new setup method.
    
    Add after `_generate_limit_db()` method (around line 282):
    
    ```python
    def _create_announcement_parses_table(self) -> None:
        """Create announcement_parses table for storing PDF extraction results."""
        db_path = self.config_dir / "fund_status.db"
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS announcement_parses (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                announcement_date DATE NOT NULL,
                pdf_filename TEXT NOT NULL,
                parse_result TEXT,
                parse_type TEXT,
                confidence REAL,
                processed INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes for common query patterns
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_announcement_parses_ticker 
            ON announcement_parses(ticker)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_announcement_parses_processed 
            ON announcement_parses(processed)
        ''')
        
        conn.commit()
        conn.close()
    ```
    
    Then update the `download()` method to call this new method after `_generate_limit_db()` (around line 344):
    ```python
    self._generate_limit_db()
    self._create_announcement_parses_table()
    ```
  </action>
  <verify>
    Check that the method exists and is called during download
  </verify>
  <done>
    - _create_announcement_parses_table() method added
    - Table schema includes all required columns
    - Indexes created on ticker and processed fields
    - Method called during download process
  </done>
</task>

<task type="auto">
  <name>Task 2: Create limit_event_log table in downloader.py</name>
  <files>src/data/downloader.py</files>
  <action>
    Add a new method `_create_limit_event_log_table()` to RealDataDownloader class.
    
    Add after `_create_announcement_parses_table()`:
    
    ```python
    def _create_limit_event_log_table(self) -> None:
        """Create limit_event_log table for audit trail."""
        db_path = self.config_dir / "fund_status.db"
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS limit_event_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                operation TEXT NOT NULL,
                old_start DATE,
                old_end DATE,
                new_start DATE,
                new_end DATE,
                triggered_by TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Create indexes for common query patterns
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_limit_event_log_ticker 
            ON limit_event_log(ticker)
        ''')
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_limit_event_log_created 
            ON limit_event_log(created_at)
        ''')
        
        conn.commit()
        conn.close()
    ```
    
    Update the `download()` method to also call this (after announcement_parses):
    ```python
    self._create_limit_event_log_table()
    ```
  </action>
  <verify>
    Verify the method exists and is called
  </verify>
  <done>
    - _create_limit_event_log_table() method added
    - Table schema includes all required columns for audit trail
    - Indexes created on ticker and created_at fields
    - Method called during download process
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tables to generators.py for mock data</name>
  <files>src/data/generator/generators.py</files>
  <action>
    Update the `FundStatusGenerator.generate()` method to create all three tables (limit_events, announcement_parses, limit_event_log) instead of just limit_events.
    
    Current code (lines 205-215) creates only limit_events. Expand it:
    
    ```python
    # Create tables if not exists
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS limit_events (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL,
            start_date DATE NOT NULL,
            end_date DATE,
            max_amount REAL DEFAULT 100.0,
            reason TEXT
        )
    """)
    
    # Create announcement_parses table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS announcement_parses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL,
            announcement_date DATE NOT NULL,
            pdf_filename TEXT NOT NULL,
            parse_result TEXT,
            parse_type TEXT,
            confidence REAL,
            processed INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Create indexes for announcement_parses
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_announcement_parses_ticker 
        ON announcement_parses(ticker)
    """)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_announcement_parses_processed 
        ON announcement_parses(processed)
    """)
    
    # Create limit_event_log table for audit trail
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS limit_event_log (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            ticker TEXT NOT NULL,
            operation TEXT NOT NULL,
            old_start DATE,
            old_end DATE,
            new_start DATE,
            new_end DATE,
            triggered_by TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # Create indexes for limit_event_log
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_limit_event_log_ticker 
        ON limit_event_log(ticker)
    """)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_limit_event_log_created 
        ON limit_event_log(created_at)
    """)
    ```
  </action>
  <verify>
    Confirm all three CREATE TABLE statements exist in generators.py
  </verify>
  <done>
    - announcement_parses table creation added
    - limit_event_log table creation added
    - All required indexes created
    - Mock data generation creates complete schema
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. Verify CREATE TABLE statements match the required schemas
2. Confirm all indexes are created
3. Test that mock data generation creates all tables
</verification>

<success_criteria>
- announcement_parses table exists with correct schema and indexes
- limit_event_log table exists with correct schema and indexes
- Both downloader.py and generators.py create all tables
- Schema is consistent between real and mock data paths
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
