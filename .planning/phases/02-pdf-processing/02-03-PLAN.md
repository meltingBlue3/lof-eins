---
phase: 02-pdf-processing
plan: 03
type: execute
wave: 2
depends_on:
  - 02-01
  - 02-02
files_modified:
  - src/data/announcement_processor.py
  - scripts/parse_announcements.py
  - tests/test_announcement_processor.py
autonomous: true
must_haves:
  truths:
    - "Can process PDF end-to-end: extract → parse → store"
    - "Stores results in announcement_parses table"
    - "CLI tool works for single ticker and batch processing"
    - "Handles non-limit announcements gracefully"
  artifacts:
    - path: "src/data/announcement_processor.py"
      provides: "Orchestration layer for PDF processing"
      exports: ["AnnouncementProcessor", "process_pdf", "process_ticker"]
    - path: "scripts/parse_announcements.py"
      provides: "CLI tool for batch processing"
      min_lines: 80
    - path: "tests/test_announcement_processor.py"
      provides: "Integration tests"
      min_tests: 4
  key_links:
    - from: "announcement_processor.py"
      to: "announcement_parses table"
      via: "sqlite3 INSERT"
      pattern: "INSERT INTO announcement_parses"
    - from: "process_pdf()"
      to: "pdf_extractor.extract_pdf_text()"
      pattern: "extract_pdf_text"
    - from: "process_pdf()"
      to: "llm_client.parse_announcement()"
      pattern: "parse_announcement"
---

<objective>
Create orchestration layer that combines PDF extraction, LLM parsing, and database storage into a complete pipeline.

Purpose: Provide end-to-end PDF processing from raw file to structured database entry.
Output: Working `src/data/announcement_processor.py` orchestration module and CLI tool for batch processing.
</objective>

<execution_context>
@C:\Users\zhang\.config\opencode/get-shit-done/workflows/execute-plan.md
@C:\Users\zhang\.config\opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-pdf-processing/02-CONTEXT.md

## Phase 2 Context Summary

**Goal:** Store raw parse results in announcement_parses table and provide CLI tool.

**Requirements:**
- PDF-03: Store raw parse results in announcement_parses table
- INT-01: Create CLI script for processing (partial - Phase 4 will complete)

**Database Schema (from Phase 1 - announcement_parses table):**
```sql
CREATE TABLE announcement_parses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    ticker TEXT NOT NULL,
    announcement_date DATE NOT NULL,
    pdf_filename TEXT NOT NULL,
    parse_result TEXT,  -- JSON
    parse_type TEXT,
    confidence REAL,
    processed INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Prior Plans:**
- 02-01: PDF extraction module (`extract_pdf_text`)
- 02-02: LLM client (`LLMClient.parse_announcement`)

**PDF Directory Structure:**
```
data/real_all_lof/announcements/
  {ticker}/
    {YYYY-MM-DD}_{title}.pdf
```

**Example:** `160105/2024-12-06_南方基金管理股份有限公司关于旗下基金投资关联方承销证券的关联交易公告.pdf`

**Key Constraint:** Not all PDFs are purchase limit announcements — system must detect this via `is_purchase_limit_announcement` flag and store anyway for audit trail.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create announcement processor module</name>
  <files>src/data/announcement_processor.py</files>
  <action>
Create `src/data/announcement_processor.py` orchestration module:

1. **Imports**:
   ```python
   import sqlite3
   import json
   from pathlib import Path
   from datetime import datetime
   from typing import Optional
   import logging
   
   from .pdf_extractor import extract_pdf_text
   from .llm_client import LLMClient
   ```

2. **AnnouncementProcessor Class**:
   ```python
   class AnnouncementProcessor:
       def __init__(
           self, 
           db_path: Path | str,
           announcements_dir: Path | str,
           llm_client: Optional[LLMClient] = None
       ):
           self.db_path = Path(db_path)
           self.announcements_dir = Path(announcements_dir)
           self.llm_client = llm_client or LLMClient()
           self.logger = logging.getLogger(__name__)
   ```

3. **Core Method**: `process_pdf(self, ticker: str, pdf_path: Path) -> dict`
   - Extract text using `extract_pdf_text()`
   - If extraction fails, log and return error dict
   - Parse using `self.llm_client.parse_announcement()`
   - Parse filename for announcement_date: `{YYYY-MM-DD}_{title}.pdf`
   - Insert result into announcement_parses table
   - Return result dict with success status

4. **Database Method**: `_save_parse_result(...)`
   - Insert into announcement_parses:
     - ticker, announcement_date, pdf_filename
     - parse_result (JSON string of LLM output)
     - parse_type (announcement_type from LLM)
     - confidence (from LLM)
     - processed = 1
   - Use parameterized query to prevent SQL injection
   - Handle database errors gracefully

5. **Batch Method**: `process_ticker(self, ticker: str) -> dict`
   - Find all PDFs for ticker: `{announcements_dir}/{ticker}/*.pdf`
   - Iterate through PDFs
   - Call process_pdf for each
   - Track statistics: total, success, failed, skipped (not limit announcements)
   - Return summary dict

6. **Helper Methods**:
   - `_parse_date_from_filename(filename: str) -> str`: Extract YYYY-MM-DD
   - `_ticker_has_parses(ticker: str) -> bool`: Check if already processed

7. **Error Handling**:
   - Continue on individual PDF failures (don't stop batch)
   - Log all errors with context
   - Return partial results even if some fail
  </action>
  <verify>
python -c "from src.data.announcement_processor import AnnouncementProcessor, process_pdf; print('Import successful')"
  </verify>
  <done>
- Module imports without errors
- AnnouncementProcessor class exists
- process_pdf convenience function exists
- Database insertion logic implemented
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CLI script</name>
  <files>scripts/parse_announcements.py</files>
  <action>
Create `scripts/parse_announcements.py` CLI tool:

1. **Imports**: argparse, sys, logging, pathlib, os

2. **Argument Parser**:
   ```python
   parser = argparse.ArgumentParser(
       description="Parse LOF fund announcement PDFs and extract limit information"
   )
   parser.add_argument("--ticker", help="Process single ticker (e.g., 161005)")
   parser.add_argument("--all", action="store_true", help="Process all tickers")
   parser.add_argument(
       "--data-dir", 
       default="data/real_all_lof",
       help="Base data directory"
   )
   parser.add_argument(
       "--db-path",
       help="Override fund_status.db path"
   )
   parser.add_argument("--verbose", "-v", action="store_true", help="Verbose output")
   ```

3. **Main Function**:
   ```python
   def main():
       args = parser.parse_args()
       
       # Setup logging
       level = logging.DEBUG if args.verbose else logging.INFO
       logging.basicConfig(level=level, format='%(levelname)s: %(message)s')
       
       # Resolve paths
       data_dir = Path(args.data_dir)
       db_path = args.db_path or data_dir / "config" / "fund_status.db"
       announcements_dir = data_dir / "announcements"
       
       # Validate paths
       if not db_path.exists():
           print(f"Error: Database not found: {db_path}", file=sys.stderr)
           sys.exit(1)
       
       if not announcements_dir.exists():
           print(f"Error: Announcements directory not found: {announcements_dir}", file=sys.stderr)
           sys.exit(1)
       
       # Initialize processor
       processor = AnnouncementProcessor(db_path, announcements_dir)
       
       # Process based on args
       if args.ticker:
           result = processor.process_ticker(args.ticker)
           _print_result(result)
       elif args.all:
           tickers = _discover_tickers(announcements_dir)
           print(f"Processing {len(tickers)} tickers...")
           for ticker in tickers:
               print(f"\n=== {ticker} ===")
               result = processor.process_ticker(ticker)
               _print_result(result)
       else:
           parser.print_help()
           sys.exit(1)
   ```

4. **Helper Functions**:
   - `_discover_tickers(announcements_dir: Path) -> list`: List subdirectories
   - `_print_result(result: dict)`: Pretty print statistics

5. **Statistics to Display**:
   - Total PDFs found
   - Successfully extracted
   - Successfully parsed
   - Stored in database
   - Skipped (not limit announcements)
   - Failed

6. **Entry Point**:
   ```python
   if __name__ == "__main__":
       main()
   ```

7. **Usage Examples** (in docstring):
   ```bash
   # Process single ticker
   python scripts/parse_announcements.py --ticker 161005
   
   # Process all tickers
   python scripts/parse_announcements.py --all
   
   # Custom data directory
   python scripts/parse_announcements.py --ticker 161005 --data-dir ./data/custom
   ```
  </action>
  <verify>
python scripts/parse_announcements.py --help
  </verify>
  <done>
- CLI script exists and runs
- --help shows all options
- Argument parsing works
- Usage examples in docstring
  </done>
</task>

<task type="auto">
  <name>Task 3: Create integration tests</name>
  <files>tests/test_announcement_processor.py</files>
  <action>
Create `tests/test_announcement_processor.py` integration tests:

1. **Test Setup**:
   ```python
   import unittest
   import tempfile
   import sqlite3
   from pathlib import Path
   from unittest.mock import Mock, patch
   
   from src.data.announcement_processor import AnnouncementProcessor
   ```

2. **Test Class**: `TestAnnouncementProcessor(unittest.TestCase)`

3. **setUp Method**:
   - Create temporary directory
   - Create mock SQLite database with announcement_parses table
   - Create mock announcements directory structure
   - Create mock PDF files (empty files are fine for testing)
   - Initialize AnnouncementProcessor with mocked LLM client

4. **Test Cases**:
   - `test_process_pdf_success`: Mock successful extraction and parsing
     - Mock extract_pdf_text to return success
     - Mock LLM client to return valid parse result
     - Verify database entry created
   
   - `test_process_pdf_extraction_failure`: Test extraction error handling
     - Mock extract_pdf_text to return failure
     - Verify graceful handling, no database entry
   
   - `test_process_pdf_not_limit_announcement`: Test non-limit handling
     - Mock LLM to return `is_purchase_limit_announcement: false`
     - Verify still stored in database (for audit trail)
   
   - `test_process_ticker_batch`: Test batch processing
     - Create multiple mock PDFs
     - Verify all processed
     - Verify statistics correct
   
   - `test_date_extraction_from_filename`: Test date parsing
     - Test various filename formats
     - Verify correct date extraction
   
   - `test_database_insertion_format`: Verify JSON storage
     - Check parse_result is valid JSON
     - Check all fields stored correctly

5. **tearDown Method**:
   - Clean up temporary directory

6. **Run Tests**:
   - Ensure all tests pass
  </action>
  <verify>
python -m pytest tests/test_announcement_processor.py -v
  </verify>
  <done>
- At least 6 test cases defined
- All tests pass
- Tests use mocking to avoid requiring real PDFs/LLM
- Database operations verified
  </done>
</task>

<task type="auto">
  <name>Task 4: Integration verification and smoke test</name>
  <files>src/data/announcement_processor.py, scripts/parse_announcements.py</files>
  <action>
1. **End-to-End Verification**:
   - Check that imports work correctly
   - Verify AnnouncementProcessor can be instantiated
   - Test with a real ticker if PDFs exist

2. **Smoke Test**:
   ```bash
   # Test help
   python scripts/parse_announcements.py --help
   
   # Test single ticker (if 161005 has PDFs)
   python scripts/parse_announcements.py --ticker 161005 --verbose
   ```

3. **Database Verification**:
   - Query announcement_parses table to verify inserts work
   - Check JSON format of parse_result

4. **Documentation**:
   - Update module docstrings with usage examples
   - Document expected directory structure
   - Add troubleshooting section (Ollama not running, etc.)

5. **Requirements Check**:
   - Verify all imports are in requirements.txt
   - Ensure no missing dependencies

6. **Error Messages**:
   - Review error messages for clarity
   - Ensure they guide user to solutions
  </action>
  <verify>
python -c "
from src.data.announcement_processor import AnnouncementProcessor
from src.data.pdf_extractor import extract_pdf_text
from src.data.llm_client import LLMClient
print('All imports successful')
"
  </verify>
  <done>
- All modules import correctly
- CLI --help works
- Integration smoke test passes
- Documentation complete
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. Run `python -m pytest tests/test_announcement_processor.py -v` - all tests pass
2. Run `python scripts/parse_announcements.py --help` - shows usage
3. Test with real ticker: `python scripts/parse_announcements.py --ticker 161005`
4. Verify database entries created with correct JSON format
5. Check that non-limit announcements are handled gracefully
</verification>

<success_criteria>
- [ ] `src/data/announcement_processor.py` exists with `AnnouncementProcessor` class
- [ ] `process_pdf()` method extracts, parses, and stores in one call
- [ ] `process_ticker()` method handles batch processing
- [ ] `scripts/parse_announcements.py` CLI works with --ticker and --all
- [ ] Results stored in announcement_parses table with correct schema
- [ ] Integration tests pass (≥6 tests)
- [ ] Can process at least one real PDF end-to-end
- [ ] Non-limit announcements detected and stored with flag
</success_criteria>

<output>
After completion, create `.planning/phases/02-pdf-processing/02-03-SUMMARY.md`
</output>
