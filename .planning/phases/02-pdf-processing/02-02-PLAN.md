---
phase: 02-pdf-processing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/data/llm_client.py
  - tests/test_llm_client.py
autonomous: true
user_setup:
  - service: ollama
    why: "Local LLM for parsing PDF content"
    install:
      - task: "Install Ollama"
        command: "Visit https://ollama.com and download for your OS"
      - task: "Pull model"
        command: "ollama pull qwen2.5:7b"  # Good for Chinese
    verify: "ollama --version && ollama list"
must_haves:
  truths:
    - "Can call Ollama API to parse PDF text"
    - "Returns structured JSON with limit information"
    - "Handles four announcement types correctly"
    - "Provides confidence score for each extraction"
  artifacts:
    - path: "src/data/llm_client.py"
      provides: "Ollama API client with structured output"
      exports: ["parse_announcement", "LLMClient", "LLMError"]
    - path: "tests/test_llm_client.py"
      provides: "Unit tests for LLM client"
      min_tests: 4
  key_links:
    - from: "parse_announcement()"
      to: "requests.post(ollama_api)"
      pattern: "requests\.post.*ollama"
---

<objective>
Implement Ollama LLM client for parsing fund announcement text and extracting structured limit information.

Purpose: Send extracted PDF text to local LLM and get structured JSON with purchase limit details.
Output: Working `src/data/llm_client.py` with robust prompt engineering and structured output parsing.
</objective>

<execution_context>
@C:\Users\zhang\.config\opencode/get-shit-done/workflows/execute-plan.md
@C:\Users\zhang\.config\opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-pdf-processing/02-CONTEXT.md

## Phase 2 Context Summary

**Goal:** Parse limit information using local LLM (Ollama) with structured JSON output.

**Requirements:**
- PDF-02: Parse limit information using local LLM
- Design extraction prompt for four announcement types
- Extract: has_limit_info, limit_type, start_date, end_date, max_amount, confidence

**Output Schema (from CONTEXT.md):**
```json
{
  "ticker": "string or null",
  "limit_amount": "number or null",
  "start_date": "YYYY-MM-DD or null",
  "end_date": "YYYY-MM-DD or null",
  "announcement_type": "complete|open-start|end-only|modify|null",
  "is_purchase_limit_announcement": "boolean",
  "confidence": "number 0-1"
}
```

**Announcement Types:**
1. **Complete**: Has both start_date and end_date
2. **Open-start**: Only has end_date (limit already active)
3. **End-only**: Announces end of existing limit
4. **Modify**: Changes existing limit parameters

**Domain Insight:** Not all PDFs are purchase restriction announcements — system must gracefully detect unrelated PDFs and set `is_purchase_limit_announcement: false`.

**Few-shot Examples:** Include 2-3 examples covering all types in the prompt.

**Prior Decision:** Use local LLM (Ollama) for cost control, data privacy, and offline capability.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM client module</name>
  <files>src/data/llm_client.py</files>
  <action>
Create `src/data/llm_client.py` with the following components:

1. **Imports**: requests, json, os, logging, typing, re, datetime

2. **Configuration Constants**:
   ```python
   DEFAULT_OLLAMA_URL = "http://localhost:11434"
   DEFAULT_MODEL = "qwen2.5:7b"  # Good for Chinese
   ```

3. **Custom Exception**:
   ```python
   class LLMError(Exception):
       """Raised when LLM API call fails."""
       pass
   ```

4. **LLMClient Class**:
   ```python
   class LLMClient:
       def __init__(self, base_url: str = None, model: str = None):
           self.base_url = base_url or os.getenv("OLLAMA_URL", DEFAULT_OLLAMA_URL)
           self.model = model or os.getenv("OLLAMA_MODEL", DEFAULT_MODEL)
           self.session = requests.Session()
   ```

5. **Prompt Design** (Critical):
   Create `_build_prompt(text: str) -> str` method with:
   - System instruction: "You are a financial document parser..."
   - Task description: Extract purchase limit information from fund announcements
   - Output format: Strict JSON with all required fields
   - Four announcement type definitions
   - Field descriptions for each output key
   - Few-shot examples (2-3 covering complete, open-start, end-only)
   - Important note: Set `is_purchase_limit_announcement: false` if not a limit announcement

6. **Main Method**: `parse_announcement(self, text: str) -> dict`
   - Build prompt with text
   - Call Ollama API: POST /api/generate
   - Request body: model, prompt, stream=false, format="json"
   - Parse response JSON
   - Validate and clean output
   - Return structured dict

7. **Response Handling**:
   - Extract `response` field from Ollama response
   - Parse as JSON
   - Validate required fields present
   - Set defaults for missing optional fields
   - Handle JSON parsing errors gracefully

8. **Validation Helpers**:
   - `_validate_date(date_str: str) -> str | None`: Validate YYYY-MM-DD format
   - `_extract_amount(text: str) -> float | None`: Extract numeric amount from Chinese text
   - `_clean_output(raw: dict) -> dict`: Ensure all required keys exist

9. **Error Handling**:
   - Connection errors (Ollama not running)
   - Timeout handling (set reasonable timeout ~60s)
   - Malformed responses
   - JSON parsing errors
   - Return error dict instead of raising for recoverable errors
  </action>
  <verify>
python -c "from src.data.llm_client import LLMClient, LLMError, parse_announcement; print('Import successful')"
  </verify>
  <done>
- Module imports without errors
- LLMClient class exists with correct methods
- parse_announcement function exists (convenience wrapper)
- Prompt includes few-shot examples
  </done>
</task>

<task type="auto">
  <name>Task 2: Create unit tests with mocks</name>
  <files>tests/test_llm_client.py</files>
  <action>
Create `tests/test_llm_client.py` with comprehensive tests:

1. **Test Class**: `TestLLMClient(unittest.TestCase)`

2. **Mock Setup**:
   ```python
   def setUp(self):
       self.client = LLMClient()
       self.mock_response = {
           "model": "test-model",
           "response": json.dumps({
               "ticker": "161005",
               "limit_amount": 100.0,
               "start_date": "2024-01-01",
               "end_date": "2024-03-01",
               "announcement_type": "complete",
               "is_purchase_limit_announcement": True,
               "confidence": 0.95
           })
       }
   ```

3. **Test Cases**:
   - `test_parse_announcement_success`: Mock successful API call
     - Patch requests.post
     - Verify correct output structure
     - Verify all fields parsed correctly
   
   - `test_parse_announcement_not_limit`: Test non-limit announcement detection
     - Mock response with `is_purchase_limit_announcement: false`
     - Verify handling
   
   - `test_parse_announcement_connection_error`: Test connection failure
     - Mock requests.post to raise ConnectionError
     - Verify error handling
   
   - `test_parse_announcement_invalid_json`: Test malformed LLM response
     - Mock response with invalid JSON
     - Verify graceful handling
   
   - `test_prompt_building`: Verify prompt structure
     - Test _build_prompt includes examples
     - Test includes Chinese context
   
   - `test_date_validation`: Test date extraction and validation
     - Test valid dates
     - Test invalid formats
     - Test null handling

4. **Integration Test (Optional)**:
   - `test_real_ollama_call`: Only run if OLLAMA_TEST=1 env var set
   - Skip by default to avoid requiring Ollama in CI

5. **Run Tests**:
   - Ensure all tests pass
  </action>
  <verify>
python -m pytest tests/test_llm_client.py -v
  </verify>
  <done>
- At least 6 test cases defined
- All tests pass
- Tests cover success, failure, and edge cases
- Mock-based tests don't require Ollama running
  </done>
</task>

<task type="auto">
  <name>Task 3: Add requirements and create test CLI</name>
  <files>requirements.txt, src/data/llm_client.py</files>
  <action>
1. **Update requirements.txt**:
   - Ensure `requests>=2.28.0` is present (usually already there)

2. **Convenience Function**:
   - Add module-level function:
   ```python
   def parse_announcement(text: str, **kwargs) -> dict:
       """Convenience function using default client."""
       client = LLMClient(**kwargs)
       return client.parse_announcement(text)
   ```

3. **CLI Test Mode**:
   - Add `if __name__ == "__main__":` block
   - Accept text file path as argument
   - Read file, call parse_announcement
   - Print formatted JSON result
   - Usage: `python src/data/llm_client.py extracted_text.txt`

4. **Documentation**:
   - Add module docstring explaining Ollama setup
   - Add example usage in docstring
   - Document expected Ollama model (qwen2.5:7b recommended)

5. **Environment Variable Support**:
   - Document OLLAMA_URL and OLLAMA_MODEL env vars
   - Show how to configure different models
  </action>
  <verify>
python -c "import requests; print('requests version:', requests.__version__)"
  </verify>
  <done>
- requests library available
- Convenience function works
- CLI test mode implemented
- Documentation complete
  </done>
</task>

</tasks>

<verification>
After completing all tasks:
1. Run `python -m pytest tests/test_llm_client.py -v` - all tests pass
2. Verify prompt includes few-shot examples for all announcement types
3. Check that output schema matches requirements
4. Test error handling with mock failures
5. Review documentation for Ollama setup instructions
</verification>

<success_criteria>
- [ ] `src/data/llm_client.py` exists with `LLMClient` class
- [ ] `parse_announcement()` function available
- [ ] Prompt includes few-shot examples for all 4 announcement types
- [ ] Output matches required JSON schema (8 fields)
- [ ] Unit tests pass (≥6 tests with mocking)
- [ ] Error handling for connection, timeout, invalid JSON
- [ ] Documentation includes Ollama setup instructions
</success_criteria>

<output>
After completion, create `.planning/phases/02-pdf-processing/02-02-SUMMARY.md`
</output>
